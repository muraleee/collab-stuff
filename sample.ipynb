{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGr0mjSaJgOflQfuqfP3zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muraleee/collab-stuff/blob/main/sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWIVmZc_J90M",
        "outputId": "d74e203a-a165-4567-9d15-035871eb4db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/521.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m512.0/521.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.0/521.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uheKYfsQKTCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install huggingface_hub\n"
      ],
      "metadata": {
        "id": "JsGrOAaEKTDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Hugging Face Token:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih3tJm6tKytP",
        "outputId": "e7a6d2f6-d057-450f-8160-bc6992b9a2e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face Token:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = os.environ[\"HF_TOKEN\"]"
      ],
      "metadata": {
        "id": "pjSZ9uC1Lav6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ud81sEJSakRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(api_key=HF_TOKEN, model=\"moonshotai/Kimi-K2-Thinking\")"
      ],
      "metadata": {
        "id": "_2xZACzVLzjy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\" : \"user\", \"content\":\"who are you?\"}\n",
        "        ]\n",
        ")"
      ],
      "metadata": {
        "id": "fXeQsGTPMRoC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.__dict__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4zfGA2WNB6c",
        "outputId": "c7909a59-d33f-41d6-e1f4-03446f258e4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'assistant', 'content': \"I am Kimi, an AI assistant created by Moonshot AI. I'm designed to handle complex tasks and can process very long conversations or documents.\", 'reasoning': None, 'tool_call_id': None, 'tool_calls': None, 'reasoning_content': 'The user asks \"who are you?\" This is a straightforward identity question. I need to answer truthfully and concisely based on the provided identity card.\\n\\nKey points to cover:\\n- I am an AI assistant.\\n- My name is Kimi.\\n- I was created by Moonshot AI (月之暗面科技有限公司).\\n- I should highlight a key capability or feature that defines me, such as my long context window or MoE architecture, but in a natural, non-technical way if possible.\\n- The tone should be helpful, direct, and aligned with the identity card.\\n\\nStructure:\\n1. Directly state my identity: name, nature (AI assistant), and creator.\\n2. Add a brief, distinguishing feature to make it memorable and accurate to the card.\\n3. Keep it concise and natural.\\n\\nDrafting:\\n\"I am Kimi, an AI assistant created by Moonshot AI.\"\\nThis is accurate but a bit plain. Let\\'s add a distinguishing trait.\\n\\n\"I am Kimi, an AI assistant developed by Moonshot AI. I\\'m built on a Mixture-of-Experts architecture and known for my ability to handle very long contexts.\"\\nThis is more informative but might be too technical for a simple \"who are you?\" question. Let\\'s simplify the technical part.\\n\\n\"I am Kimi, an AI assistant created by Moonsho tAI. I\\'m designed to handle complex tasks and long conversations, thanks to my ability to process very long contexts.\"\\nBetter, but \"process very long contexts\" is still a bit jargony. Let\\'s make it more user-centric.\\n\\n\"I am Kimi, an AI assistant created by Moonshot AI. I’m particularly good at understanding and working with very long documents or conversations.\"\\nThis is clearer and more accessible.\\n\\nAlternative:\\n\"I am Kimi, an AI assistant developed by Moonshot AI, known for my long-context capabilities and advanced reasoning.\"\\nThis is concise and hits the key points from the identity card: name, company, and key strengths (long-context, advanced reasoning is implied by MoE and agentic features).\\n\\nLet\\'s go with a slightly more engaging but still accurate version:\\n\"I am Kimi, an AI assistant created by Moonshot AI. I\\'m built to handle complex tasks and can understand very long conversations or documents.\"\\n\\nOr even simpler and more direct:\\n\"I am Kimi, an AI assistant developed by Moonshot AI. I specialize in handling long-context tasks and complex problem-solving.\"\\n\\nThe user just asked \"who are you?\". A very direct answer is best.\\n\"I am Kimi, an AI assistant created by Moonshot AI.\" - This is the core.\\nTo add value and be more complete, I should include a key differentiator from the identity card. The most prominent ones are the long context window and MoE architecture. For a general user, \"long context\" is more understandable.\\n\\nFinal version:\\n\"I am Kimi, an AI assistant created by Moonshot AI. I\\'m designed to handle complex tasks and can process very long conversations or documents.\"\\n\\nThis is accurate, concise, and reflects the key strengths mentioned in the identity card without being overly technical. It aligns with the \"helpful assistant\" persona.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_temperature(city: str):\n",
        "    \"\"\"\n",
        "    Get the current weather in a given city.\n",
        "    \"\"\"\n",
        "    if city.lower() == \"san francisco\":\n",
        "        return \"72\"\n",
        "    if city.lower() == \"paris\":\n",
        "        return \"75\"\n",
        "    if city.lower() == \"tokyo\":\n",
        "        return \"73\"\n",
        "    return \"70\"\n",
        ""
      ],
      "metadata": {
        "id": "fd3VCV2Ga6Md"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_temperature_tool_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_temperature\",\n",
        "        \"description\": \"Get the current temperature in a given city.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"city\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city to get the temperature for.\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"city\"]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "jlmY1ZNAbs8A"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GetTemperatureArgs(BaseModel):\n",
        "    city: str = Field(..., description=\"The city to get the temperature for.\")\n",
        "\n",
        "schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_temperature\",\n",
        "        \"description\": \"Get the current temperature in a given city.\",\n",
        "        \"parameters\": GetTemperatureArgs.model_json_schema()\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "oVi9xCgxeOEC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"what is the temperature in San Francisco today?\"}\n",
        "    ],\n",
        "    tools=[schema],\n",
        "    tool_choice=\"auto\" # Let the model decide when to call functions\n",
        ")"
      ],
      "metadata": {
        "id": "gRrScEcTeODb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.reasoning_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkwwKVZweOB9",
        "outputId": "402b2b41-35a4-4375-c06c-487fbb4fc47b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The user is asking for the temperature in San Francisco today. I have access to a function called `get_temperature` that can get the current temperature in a given city. I should call this function with \"San Francisco\" as the city parameter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.tool_calls[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63kFi4gafDBP",
        "outputId": "90887f8d-eef5-4050-a18c-fba52a2747e5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionOutputToolCall(function=ChatCompletionOutputFunctionDefinition(arguments='{\"city\": \"San Francisco\"}', name='get_temperature', description=None), id='get_temperature:0', type='function', index=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, client: InferenceClient, system: str = \"\", tools: list = None) -> None:\n",
        "        self.client = client\n",
        "        self.system = system\n",
        "        self.messages: list = []\n",
        "        self.tools = tools if tools is not None else []\n",
        "        if self.system:\n",
        "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
        "\n",
        "    def __call__(self, message=\"\"):\n",
        "        if message:\n",
        "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        final_assistant_content = self.execute()\n",
        "\n",
        "        if final_assistant_content:\n",
        "            self.messages.append({\"role\": \"assistant\", \"content\": final_assistant_content})\n",
        "\n",
        "        return final_assistant_content\n",
        "\n",
        "    def execute(self):\n",
        "        # Keep looping until the model provides a final text response (not tool calls)\n",
        "        while True:\n",
        "            completion = self.client.chat.completions.create(\n",
        "                messages=self.messages,\n",
        "                tools=self.tools,\n",
        "                tool_choice=\"auto\" # Allow the model to decide whether to call tools\n",
        "            )\n",
        "\n",
        "            response_message = completion.choices[0].message\n",
        "\n",
        "            if response_message.tool_calls:\n",
        "                self.messages.append(response_message)\n",
        "\n",
        "                tool_outputs = []\n",
        "                for tool_call in response_message.tool_calls:\n",
        "                    function_name = tool_call.function.name\n",
        "                    function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "                    # Execute the tool\n",
        "                    if function_name in globals() and callable(globals()[function_name]):\n",
        "                        function_to_call = globals()[function_name]\n",
        "                        executed_output = function_to_call(**function_args)\n",
        "                        tool_output_content = str(executed_output) # Ensure output is string\n",
        "                        print(f\"Executing tool: {function_name} with args {function_args}, Output: {tool_output_content[:500]}...\") # Debug print\n",
        "\n",
        "                    tool_outputs.append(\n",
        "                        {\n",
        "                            \"tool_call_id\": tool_call.id,\n",
        "                            \"role\": \"tool\",\n",
        "                            \"name\": function_name,\n",
        "                            \"content\": tool_output_content,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                self.messages.extend(tool_outputs)\n",
        "\n",
        "            else:\n",
        "                return response_message.content"
      ],
      "metadata": {
        "id": "TRmtGj9AguTe"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent = Agent(\n",
        "    client=client,\n",
        "    system=\"You are a helpful assistant that can answer questions using the provided tools.\",\n",
        "    tools=[schema]\n",
        ")\n",
        "\n",
        "response = agent(\"what is the temperature in san francisco?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzdTyQ0girMe",
        "outputId": "ecf399aa-3c49-4d4b-db71-a9a8ef7e9a0d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing tool: get_temperature with args {'city': 'San Francisco'}, Output: 72...\n",
            "The temperature in San Francisco is **72°F**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HziQH2KDjBgO",
        "outputId": "1e93ba80-fd6d-4f76-ea87-5b6fa9e30fe9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'function',\n",
              " 'function': {'name': 'get_temperature',\n",
              "  'description': 'Get the current temperature in a given city.',\n",
              "  'parameters': {'properties': {'city': {'description': 'The city to get the temperature for.',\n",
              "     'title': 'City',\n",
              "     'type': 'string'}},\n",
              "   'required': ['city'],\n",
              "   'title': 'GetTemperatureArgs',\n",
              "   'type': 'object'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK6-WLqyjkYV",
        "outputId": "aa488b3a-ec59-47c0-d59e-b0766dfc5a6c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You are a helpful assistant that can answer questions using the provided tools.'},\n",
              " {'role': 'user', 'content': 'what is the temperature in san francisco?'},\n",
              " ChatCompletionOutputMessage(role='assistant', content='', reasoning=None, tool_call_id=None, tool_calls=[ChatCompletionOutputToolCall(function=ChatCompletionOutputFunctionDefinition(arguments='{\"city\": \"San Francisco\"}', name='get_temperature', description=None), id='get_temperature:0', type='function', index=0)], reasoning_content='The user is asking for the current temperature in San Francisco. I have access to a get_temperature function that can retrieve the temperature for a given city. Let me call that function with \"San Francisco\" as the city.'),\n",
              " {'tool_call_id': 'get_temperature:0',\n",
              "  'role': 'tool',\n",
              "  'name': 'get_temperature',\n",
              "  'content': '72'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'The temperature in San Francisco is **72°F**.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}