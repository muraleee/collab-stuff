{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOtq7ny6vKhWy0t52aO7FGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muraleee/collab-stuff/blob/main/whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "lKalno4D2qs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruHgpXjHgTIc"
      },
      "outputs": [],
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lDbVKSJjv8Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O SL-e01-Part1.mp3 https://raw.githubusercontent.com/muraleee/saundlaha/refs/heads/main/SL-e01-Part1.mp3\n"
      ],
      "metadata": {
        "id": "Jylaeom8pzh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3p18yC6_d16P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_EUS-s_d2Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n"
      ],
      "metadata": {
        "id": "y25jIxJ_qNnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a46a47-13d9-41c9-c342-e02d8f6181b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bqKv2nfATqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import whisper\n",
        "# import os\n",
        "# import time\n",
        "\n",
        "# def transcribe_with_stats(file_path, model_type=\"large\"):\n",
        "#     if not os.path.exists(file_path):\n",
        "#         print(f\"Error: {file_path} not found.\")\n",
        "#         return\n",
        "\n",
        "#     # Load model\n",
        "#     print(f\"Loading Whisper {model_type} model...\")\n",
        "#     model = whisper.load_model(model_type)\n",
        "\n",
        "#     # Start Timer\n",
        "#     start_time = time.perf_counter()\n",
        "\n",
        "#     print(f\"Transcribing: {file_path}...\")\n",
        "#     try:\n",
        "#         result = model.transcribe(file_path)\n",
        "\n",
        "#         # End Timer\n",
        "#         end_time = time.perf_counter()\n",
        "#         total_seconds = end_time - start_time\n",
        "\n",
        "#         # Save to file\n",
        "#         base_name = os.path.splitext(file_path)[0]\n",
        "#         output_file = f\"{base_name}.txt\"\n",
        "#         with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#             f.write(result[\"text\"].strip())\n",
        "\n",
        "#         # Format and display time\n",
        "#         mins, secs = divmod(total_seconds, 60)\n",
        "#         print(\"-\" * 30)\n",
        "#         print(f\"‚úÖ Success!\")\n",
        "#         print(f\"üìÑ Saved to: {output_file}\")\n",
        "#         print(f\"‚è±Ô∏è Time taken: {int(mins)}m {secs:.2f}s\")\n",
        "#         print(\"-\" * 30)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     audio_file = \"./saundlaha/SL-e01-Part5.mp3\"\n",
        "#     transcribe_with_stats(audio_file)"
      ],
      "metadata": {
        "id": "3WfboyqV3DWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "\n",
        "def transcribe_saundarya_lahari(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: {file_path} not found.\")\n",
        "        return\n",
        "\n",
        "    # Use 'large' or 'turbo' for best Sanskrit phonetics\n",
        "    model_type = \"large\"\n",
        "    print(f\"Loading {model_type} model...\")\n",
        "    model = whisper.load_model(model_type)\n",
        "\n",
        "    # Specialized Prompt for Saundarya Lahari\n",
        "    # Includes specific mantras and names to prime the AI\n",
        "    sl_prompt = (\n",
        "        \"This is a discourse on the Saundarya Lahari Text authored by Adi Shankaracharya. \"\n",
        "        \"It includes Sanskrit verses, Shlokas, and terms like Devi, Devata, Chandrasekhar, Tanras, Vedas and many other Indian spiritual terms from Sanskrit\"\n",
        "        \"The speaker has an Indian accent. Additionally,\"\n",
        "        \"there are parts of sanskrit verse fragments in the audio they maybe interspersed between English text\"\n",
        "\n",
        "    )\n",
        "\n",
        "    print(f\"Transcribing: {file_path}...\")\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    try:\n",
        "        result = model.transcribe(\n",
        "                audio=file_path,\n",
        "                language=\"en\",                        # Stick to English commentary\n",
        "                initial_prompt=sl_prompt,\n",
        "                word_timestamps=True,                 # Pinpoint the verses exactly\n",
        "                beam_size=5,                          # Higher accuracy for technical terms\n",
        "                condition_on_previous_text=False      # Prevents \"looping\" on complex Sanskrit\n",
        "            )\n",
        "        elapsed = time.perf_counter() - start_time\n",
        "\n",
        "        # Save output\n",
        "        output_file = os.path.splitext(file_path)[0] + \".txt\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(result[\"text\"].strip())\n",
        "\n",
        "        output_json = os.path.splitext(file_path)[0] + \".json\"\n",
        "\n",
        "        with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
        "            # indent=4 makes the file human-readable\n",
        "            # ensure_ascii=False keeps Devanagari/Sanskrit characters intact\n",
        "            json.dump(result, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"‚úÖ Completed in {elapsed/60:.1f} minutes.\")\n",
        "        print(f\"üìÑ Text saved to: {output_file}\")\n",
        "        print(f\"üìÑ Text in JSON saved to: {output_json}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    audio_file = \"./saundlaha/SL-e01-Part5.mp3\"\n",
        "    transcribe_saundarya_lahari(audio_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHwMjRadBDaV",
        "outputId": "893f851d-89b9-4856-846b-2dc07abf4e5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading large model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:14<00:00, 220MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing: ./saundlaha/SL-e01-Part5.mp3...\n",
            "‚úÖ Completed in 3.5 minutes.\n",
            "üìÑ Text saved to: ./saundlaha/SL-e01-Part5.txt\n",
            "üìÑ Text in JSON saved to: ./saundlaha/SL-e01-Part5.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import os\n",
        "# from transformers import pipeline\n",
        "\n",
        "# # 1. Environment & Device Setup\n",
        "# # H100/A100/L4 benefit from float16 precision\n",
        "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# model_id = \"ARTPARK-IISc/whisper-large-v3-vaani-hindi\"\n",
        "\n",
        "# print(f\"Loading model on {device}...\")\n",
        "# transcribe = pipeline(\n",
        "#     task=\"automatic-speech-recognition\",\n",
        "#     model=model_id,\n",
        "#     device=device,\n",
        "#     torch_dtype=torch_dtype,\n",
        "#     model_kwargs={\"attn_implementation\": \"sdpa\"} # Faster attention for A100/H100/L4\n",
        "# )\n",
        "\n",
        "# # 2. Prepare specialized prompt\n",
        "# audio_file = \"./saundlaha/SL-e01-Part2.mp3\"\n",
        "# sl_prompt = (\n",
        "#     \"This is a discourse on the Saundarya Lahari by Adi Shankaracharya. ‡§∂‡§ø‡§µ‡§É ‡§∂‡§ï‡•ç‡§§‡•ç‡§Ø‡§æ ‡§Ø‡•Å‡§ï‡•ç‡§§‡•ã \"\n",
        "#     \"It includes Sanskrit verses, Shlokas, and terms like Devi, Devata, Chandrasekhar, \"\n",
        "#     \"Chakras, Tantras, and Vedas. The speaker has an Indian accent.\"\n",
        "# )\n",
        "\n",
        "# # Fix: Convert prompt IDs to PyTorch Tensor to avoid NumPy TypeError\n",
        "# prompt_ids = transcribe.tokenizer.get_prompt_ids(sl_prompt, return_tensors=\"pt\")\n",
        "# prompt_ids = prompt_ids.to(device)\n",
        "\n",
        "# print(f\"Transcribing: {audio_file}...\")\n",
        "# result = transcribe(\n",
        "#     audio_file,\n",
        "#     chunk_length_s=30,\n",
        "#     generate_kwargs={\"prompt_ids\": prompt_ids}\n",
        "# )\n",
        "\n",
        "# # 3. Save to file\n",
        "# output_dir = \"./saundlaha/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "# output_file = os.path.join(output_dir, \"Vaani_Part2.txt\")\n",
        "\n",
        "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#     f.write(result[\"text\"])\n",
        "\n",
        "# print(f\"‚úÖ Success! Transcription saved to {output_file}\")"
      ],
      "metadata": {
        "id": "mz-DHpB3Lj0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}